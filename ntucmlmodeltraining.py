# -*- coding: utf-8 -*-
"""ntucMLmodeltraining.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CzXoOdBco1lvJLQTkNWH_eP-AuufOsbF
"""

import streamlit as st
import pandas as pd
from surprise import Dataset, Reader, KNNBasic
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# Install the packages listed in requirements.txt
requirements_file_path = "requirements.txt"
with open(requirements_file_path, "r") as f:
    requirements = f.read().splitlines()
for requirement in requirements:
    st.write(f"Installing {requirement}...")
    st.run(f"pip install {requirement}")

# Upload data from an Excel file
file_path = r"C:\Users\adminnus\Desktop\NTUCprototype\ntucMLdata.xlsx"
data = pd.read_excel(file_path)

# ... rest of the Streamlit app code ...

"""The modules you may want to install depend on the specific requirements of your project and the tasks you want to perform. Since you are working on a recommender system and data analysis, here are some common Python modules that you might find useful:

scikit-learn: This library provides a wide range of machine learning algorithms, including collaborative filtering and other recommender system techniques.

seaborn: A data visualization library based on matplotlib that provides a high-level interface for creating informative and attractive statistical graphics.

xlsxwriter: A library for creating Excel XLSX files.

openpyxl: A library to read/write Excel files (XLSX).

numpy: A powerful library for numerical computing in Python.

pandas: A versatile library for data manipulation and analysis.

matplotlib: A popular library for creating static, interactive, and animated plots and visualizations in Python.

plotly: A graphing library that makes interactive, publication-quality graphs online.

scipy: A library for scientific computing in Python, including optimization, integration, linear algebra, and more.

surprise: A Python scikit for building and analyzing recommender systems.

xlrd: A library for reading data and formatting information from Excel files (XLS).

To install these modules, you can use the !pip install command in Google Colab
"""

#IGNORE THIS AND UPLOAD TO GOOGLE COLABS ENVIRONMENT AND USE THE ENVIRONMENT FILE PATH IN THE NEXT CODE SEGMENT

# Function to upload data from Excel
def upload_data_from_excel(file_path):
    data = pd.read_excel(file_path)
    return data

# Upload data from an Excel file
file_path = r'C:\Users\adminnus\Desktop\NTUCprototype\ntucMLdata.xlsx'  # Replace with the actual file path
data = upload_data_from_excel(file_path)

from google.colab import files

# Upload the "ntucMLdata.xlsx" file
uploaded = files.upload()

# Get the path to the uploaded file
file_path = next(iter(uploaded))

print(file_path)

import pandas as pd
from surprise import Dataset, Reader, KNNBasic
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt


# Function to upload data from Excel
def upload_data_from_excel(file_path):
    data = pd.read_excel(file_path)
    return data

# Upload data from an Excel file
file_path = r'ntucMLdata (3).xlsx'  # Replace with the actual file path
data = upload_data_from_excel(file_path)

# Function to get predictions for all user-item pairs
def get_predictions(model, users, items, user_params, activity_params):
    predictions = []
    for i, user in enumerate(users):
        user_params_i = user_params[i]
        user_predictions = [
            model.predict(user, item, user_params_i, activity_params[item]).est
            for item in items
        ]
        predictions.append(user_predictions)
    return np.array(predictions)

# Function to generate recommendation lists for each user
def generate_recommendation_lists(predictions, users, items, top_n=5):
    recommendation_lists = {}
    for i, user in enumerate(users):
        top_activity_indices = np.argsort(predictions[i])[::-1][:top_n]
        top_activities = [items[idx] for idx in top_activity_indices]
        recommendation_lists[user] = top_activities
    return recommendation_lists

# Function to generate user-item heatmap
def generate_user_item_heatmap(predictions, users, items):
    plt.figure(figsize=(12, 8))
    sns.heatmap(predictions, cmap="YlGnBu", linewidths=0.5, annot=True, fmt=".1f",
                xticklabels=items, yticklabels=users)
    plt.title("User-Item Heatmap")
    plt.xlabel("Activity")
    plt.ylabel("User")
    plt.show()


# Get all unique users and items
users = data['user'].unique()
items = data['activity'].unique()

# Additional parameters for users and activities
user_params = data.groupby('user')[['UserDifficultyLevel', 'UserSocialActivity', 'UserPhysicalIntensity']].mean().values
activity_params = data.groupby('activity')[['ActivityDifficultyLevel', 'ActivitySocialActivity', 'ActivityPhysicalIntensity']].mean().to_dict('index')

# Create a Surprise Dataset from the DataFrame
reader = Reader(rating_scale=(1, 5))
data = Dataset.load_from_df(data[['user', 'activity', 'rating']], reader)

# Train the KNNBasic collaborative filtering model
trainset = data.build_full_trainset()
model = KNNBasic()
model.fit(trainset)

# Get predictions for all user-item pairs
all_predictions = get_predictions(model, users, items, user_params, activity_params)

# Convert predictions to DataFrame for easier manipulation
predictions_df = pd.DataFrame(all_predictions, index=users, columns=items)

# Generate recommendation lists for each user (top 3 recommendations)
recommendation_lists = generate_recommendation_lists(predictions_df.values, users, items, top_n=3)

# Display recommendation lists
for user, recommendations in recommendation_lists.items():
    print(f"User {user}: {recommendations}")

# Generate user-item heatmap
generate_user_item_heatmap(predictions_df.values, users, items)

import pandas as pd
from surprise import Dataset, Reader, KNNBasic
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt


# Function to upload data from Excel
def upload_data_from_excel(file_path):
    data = pd.read_excel(file_path)
    return data

# Upload data from an Excel file
file_path = r'ntucMLdata (3).xlsx'  # Replace with the actual file path
data = upload_data_from_excel(file_path)

# Function to get predictions for all user-item pairs
def get_predictions(model, users, items, user_params, activity_params):
    predictions = []
    for i, user in enumerate(users):
        user_params_i = user_params[i]
        user_predictions = [
            model.predict(user, item, user_params_i, activity_params[item]).est
            for item in items
        ]
        predictions.append(user_predictions)
    return np.array(predictions)

# Function to generate recommendation lists for each user
def generate_recommendation_lists(predictions, users, items, top_n=5):
    recommendation_lists = {}
    for i, user in enumerate(users):
        top_activity_indices = np.argsort(predictions[i])[::-1][:top_n]
        top_activities = [items[idx] for idx in top_activity_indices]
        recommendation_lists[user] = top_activities
    return recommendation_lists

# Function to generate user-item heatmap
def generate_user_item_heatmap(predictions, users, items):
    plt.figure(figsize=(12, 8))
    sns.heatmap(predictions, cmap="YlGnBu", linewidths=0.5, annot=True, fmt=".1f",
                xticklabels=items, yticklabels=users)
    plt.title("User-Item Heatmap")
    plt.xlabel("Activity")
    plt.ylabel("User")
    plt.show()


# Get all unique users and items
users = data['user'].unique()
items = data['activity'].unique()

# Additional parameters for users and activities
user_params = data.groupby('user')[['UserDifficultyLevel', 'UserSocialActivity', 'UserPhysicalIntensity']].mean().values
activity_params = data.groupby('activity')[['ActivityDifficultyLevel', 'ActivitySocialActivity', 'ActivityPhysicalIntensity']].mean().to_dict('index')

# Create a Surprise Dataset from the DataFrame
reader = Reader(rating_scale=(1, 10))
data = Dataset.load_from_df(data[['user', 'activity', 'rating']], reader)

# Train the KNNBasic collaborative filtering model
trainset = data.build_full_trainset()
model = KNNBasic()
model.fit(trainset)

# Get predictions for all user-item pairs
all_predictions = get_predictions(model, users, items, user_params, activity_params)

# Scale the predictions from (1, 5) to (1, 10)
scaled_predictions = (all_predictions - 1) * 2 + 1

# Convert predictions to DataFrame for easier manipulation
predictions_df = pd.DataFrame(scaled_predictions, index=users, columns=items)

# Generate recommendation lists for each user (top 3 recommendations)
recommendation_lists = generate_recommendation_lists(predictions_df.values, users, items, top_n=3)

# Display recommendation lists
for user, recommendations in recommendation_lists.items():
    print(f"User {user}: {recommendations}")

# Generate user-item heatmap
generate_user_item_heatmap(predictions_df.values, users, items)


# Display the recommendation lists
st.header("Recommendation Lists")
for user, recommendations in recommendation_lists.items():
    st.write(f"User {user}: {recommendations}")
